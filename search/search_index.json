{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Harvard\u2019s Trusted Research Environment (TRE) is a controlled platform for analysis of sensitive data. It combines workflow provenance, secure infrastructure, and reproducible research practices. The environment aligns with the Five Safes model and with the European concept of Secure Processing Environments for secondary use of health data.</p> <p>What the TRE provides - Controlled access to data through authenticated, role-based entry. - Reproducible analysis by recording code, parameters, environments, and outputs. - Collaboration using standard tools (e.g., RStudio, Jupyter, SAS, MATLAB) within a secured boundary. - Output checking prior to release to protect confidentiality and comply with policy. - Deployment patterns for pilots, institutional services, and federated collaboration.</p> <p>Process, at a glance 1. Project onboarding and role assignment. 2. Workspace provision with curated datasets. 3. Workflow execution with automatic provenance capture. 4. Output review and disclosure control. 5. Publication or approved export.</p> <p></p> Technical detail <ul> <li>Workflow descriptions (e.g., CWL/Nextflow) and W3C PROV lineage support verification and re-run.</li> <li>Versioning of datasets, code, containers, and configurations.</li> <li>Encryption in transit and at rest; multi-factor authentication; role-based access control.</li> <li>Comprehensive logging and monitoring.</li> </ul> <p>Next - See Trust for provenance and compliance. - See Research for tools and lifecycle. - See Deployment Blueprint for implementation options. - See Project Overview for an example in use.</p>"},{"location":"environment/deployment-blueprint/","title":"Deployment Blueprint","text":"<p>The TRE can be deployed as a pilot for a single project, as an institutional service, or as a federated arrangement across organisations.</p> <p>Reference components - Compute (on-demand and batch), secure storage tiers, and network isolation. - Identity federation and single sign-on; role-based permissions. - Logging and monitoring; backup and disaster recovery. - Cost and capacity management.</p> <p>Decision table</p> Scenario Footprint Notes Pilot / small team Single-project Fast start; limited operations overhead. Many teams / shared service Institutional multi-tenant Central governance; quotas; standard toolchain. Cross-institution collaboration Federated TREs Policy alignment; shared governance; no raw data movement. Operations summary <ul> <li>Monitoring, incident response, and security patching.</li> <li>Data lifecycle management: ingest, curate, archive; retention policies.</li> <li>Performance tuning and capacity planning.</li> </ul>"},{"location":"reference-case/project-overview/","title":"Project Overview","text":"<p>This case study illustrates TRE use for climate and health integration.</p> <p>Context - Integration of climate, air quality, demographics, and claims data. - Multi-stage workflow; each step logged as lineage. - Outputs reviewed before release.</p> <p>Why it is representative - End-to-end transparency and reproducibility checks. - Reduced manual effort for compliance documentation. - Collaboration without raw data export.</p> <p></p> Workflow sketch <p>Raw sources \u2192 ingest and quality checks \u2192 harmonisation \u2192 analysis and validation \u2192 output review \u2192 publication.</p>"},{"location":"research/","title":"Research","text":"<p>Researchers use familiar tools within the TRE. The environment provides a secured workspace and a workflow lifecycle that records methods and results for verification.</p> <p>Research tools (examples) - Interactive tools: RStudio, Jupyter. - Statistical packages: SAS, Stata, MATLAB. - Terminal and batch execution for pipelines. - Single sign-on and session management.</p> <p></p> <p>Lifecycle tasks - Project registration and eligibility checks. - Data ingestion and curation with quality controls. - Workflow execution with automatic provenance capture. - Reproducibility and validation. - Output review and publication or approved export.</p> <p></p> Under the hood <ul> <li>Workflow orchestration integrates with provenance capture.</li> <li>Parameterized pipelines enable consistent, re-runnable analysis.</li> <li>Result artifacts include metadata for discovery and audit.</li> </ul>"},{"location":"resources/","title":"Resources","text":"<p>TODO: write content for Resources.</p> <ul> <li>Placeholder generated 2025-08-09T14:03:41+02:00</li> <li>Path: <code>resources/index.md</code></li> </ul>"},{"location":"resources/compliance-matrix/","title":"Compliance Matrix","text":"<p>TODO: write content for Compliance Matrix.</p> <ul> <li>Placeholder generated 2025-08-09T14:03:41+02:00</li> <li>Path: <code>resources/compliance-matrix.md</code></li> </ul>"},{"location":"resources/documentation/","title":"Documentation","text":"<p>TODO: write content for Documentation.</p> <ul> <li>Placeholder generated 2025-08-09T14:03:41+02:00</li> <li>Path: <code>resources/documentation.md</code></li> </ul>"},{"location":"team/","title":"Team","text":"<p>Harvard Research Computing partners with Forome Team to design and operate the TRE.</p> <p>Core roles - Service governance and policy alignment. - Research engineering for environments, tools, and onboarding. - Security and compliance: access control, audit, assessments. - Collaborating partners and institutions.</p> <p>Contact - Collaboration: research-computing@harvard.edu - Technical: tre-support@forome.org</p>"},{"location":"trust/","title":"Trust","text":"<p>This page describes data provenance and compliance, including alignment with Secure Processing Environments.</p>"},{"location":"trust/#data-provenance","title":"Data Provenance","text":"<p>Data provenance is necessary for reproducibility and audit. Harvard TRE implements automatic provenance capture using workflow-based processing and standards-aligned lineage.</p> <p>Core mechanisms - Workflow-first processing (Common Workflow Language, Nextflow). - W3C PROV lineage graphs linking inputs, parameters, containers, and outputs. - Versioning of datasets, code, and computational environments. - Catalog entries for artifacts to enable re-run and verification.</p> <p>Benefits - Researchers: reproducible results without additional manual logging. - Reviewers: trace from raw data to published output. - Stewards: visibility into data quality and downstream dependencies.</p> Technical detail <ul> <li>Container-based execution (Docker/OCI) to ensure consistent environments.</li> <li>Version control for workflow definitions and configurations.</li> <li>Runtime metadata: parameters, software versions, resource usage.</li> <li>Validation frameworks for re-execution and cross-checking.</li> </ul>"},{"location":"trust/#compliance-including-spe-and-trust-frameworks","title":"Compliance (including SPE and trust frameworks)","text":"<p>Secure Processing Environments are required in Europe for secondary use of health data. The TRE design supports these requirements and the Five Safes model.</p> <p>Selected requirements and TRE features - No raw data export \u2192 controlled output review and disclosure control. - Strong access control \u2192 multi-factor authentication and role-based permissions. - Auditability \u2192 logs and lineage capture for review. - Jurisdictional control \u2192 deployable in-region to meet locality requirements.</p> <p>Trust frameworks and initiatives - Five Safes principles (People, Projects, Settings, Data, Outputs). - EOSC-ENTRUST approaches to federated trust and assurance. - Compatibility with DARE/HDR practices for trusted environments.</p> <p></p>"}]}